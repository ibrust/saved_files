Mutable or immutable extrinsic state
An initial design decision in implementing a flyweight is whether to use mutable or immutable extrinsic (variant state) objects. Immutable objects can be easily shared. But this requires the creation of new extrinsic objects whenever a change in state occurs. If the objects state only occasionally changes, or changes in recurring ways, only a small number of these immutable extrinsic objects will need to be created. If the objects properties change frequently, and in unique ways, the use of mutable extrinsic objects is preferred. With a mutable implementation, objects can share state, but will not be directly modified while in use. The mutability is for better object reuse by allowing the caching & reinitialization of old, unused objects. An API must be provided in such implementations that guarantees the return of an unused, reinitialized object. When state is highly variable, such as when most objects are unique, attempts at sharing should be abandoned. In these cases, just encapsulate the highly variable state in a lightweight decorator, provide a cache for it, bulk initialize the cache, and be done with it.

Partitioning the flyweight object
One way of implementing the extrinsic (variant) component of a flyweight object is with a Decorator and using it to wrap the intrinsic object. This is preferred for state that is more variant, as decorators can be easily applied or removed from select objects, and a single object may be wrapped with multiple arbitrary decorators at runtime. Sharing of decorators is achieved by just a common reference. For the intrinsic object, a common implementation is as a class consisting of nothing but static properties. By subclassing this class and adding further static properties, you can create a variety of invariant objects. This approach can account for state that is mostly, but not entirely invariant.

Consider a text-editing application (typical example). You might highlight a paragraph of text, bolden a few words within that text... maybe italicize just one word and increase its font size. Each modification is another layer of decoration wrapping the shared, intrinsic object. Trying to implement something like this with multiple inheritance would be a complete disaster. Note that these decorators have references to one another, allowing for sharing. Messages are propagated through the decorated layers transparently.

Now consider another example, again in a word processing application. You may have 10,000 letters but only 26 unique alphabetical characters in a document. You have state that is mostly invariant - the particular letter displayed. It's not entirely invariant, but it could be easily treated as such. In such cases of mostly invariant state, subclassing the intrinsic object and adding static properties to the subclasses is the most effective optimization technique. Most compilers actually replace common objects like these - common characters, low digit numbers - using static, optimized objects. The subclassing approach has the advantage of being compiled and optimized, and the interface with the intrinsic object is kept simple via polymorphism.

These competing examples highlight the difference between hierarchical, top-down design and compositional design. In practice, objects often do not divide up neatly into variant and invariant parts, there is some grey area where you must decide how best to split up the object. As mentioned earlier, sometimes the object state is so variable that no sharing is practical, and you should just place that state in a decorator and cache it. The combined and intelligent use of the both static subclassing and decoration is what's necessary to implement a truly memory-efficient partitioning of the object.

overview of the object retrieval system
Although the Factory for creating / reusing these flyweight objects has a simple interface, this is often a Facade where the underlying system is quite complicated. Seeing as the goal of the flyweight pattern is to preserve memory, you want to try your best to cache, share, and reuse extrinsic objects wherever possible. Sharing of the extrinsic objects can be done easily by making them immutable, as mentioned earlier. Again, there are mutable implementations where sharing is possible, you simply cannot change the state of shared objects while they're being used. Instead, such a change must issue a request for either a different type of shared object, the reinitialization of an old object (the advantage of mutability), or the creation of a new object.

The specific type of cache you use generally depends on the structure of the object - whether it's mutable or immutable, and whether it can be shared or not. There are a variety of caching & retrieval algorithms to choose from. The algorithms efficiency depends on the caches data structure, number of objects, characteristics of the objects, and the memory usage patterns of the application.

To provide global access for the creation of these flyweight objects it's best to implement the factory interface as a Singleton. This should be done in both single-threaded and multi-threaded, shared memory applications. In distributed systems an alternative option is to instantiate unique instances of the entire flyweight system on each node, including the caches, though cache utilization is a potential a problem with this approach, and you will need to be very careful when bulk-initializing objects. You could come up with more complex sharing algorithms, where certain types of objects were delegated to certain nodes. You could even implement a manager that coordinates all this. That's a discussion beyond the scope of this article, though.

Generally speaking, the retrieval algorithm begins with a request for a new object via the factory interface. The request is forwarded to an appropriate cache based on what kind of object it is; how you partitioned your object, designed your caches & so fourth. Approaches to caching

caching flyweight objects
Caching objects with highly variable state - variable to a degree there's no reason to try sharing them - can be easily achieved using a FIFO structure. Here the FIFO structure will simply maintain unused objects in the cache, with no need to search the cache or deal with miss rates. Even so, an unmaintained cache could be preferred. There's less upfront overhead associated with an unmaintained cache: you typically initialize objects for the caches in bulk at compile time or startup. Thus the initial miss rate for an unmaintained cache should be zero for quite some time, until you run out of objects, and you won't need to perform pop operations initially. On the other hand, once you do run out the object retrieval algorithm might have more overhead associated than the push/pop operations of a maintained cache. Which type of cache to prefer here really depends on the memory usage patterns of your application, the actual data structure of the cache, the number of objects you have, & above all - the results of testing your implementation. If your application will use a very consistent number of these objects, an unmaintained cache that's bulk initialized with that exact amount of objects is preferred. If the total number of objects can vary greatly than you might want to use a maintained cache for this. Ultimately only testing will tell you which implementation to go with.

If you choose to use an unmaintained cache, you need a search algorithm, and it needs to minimize misses. There are a wide variety of cache search algorithms to choose from. It is helpful to encapsulate these selection algorithms in a Strategy pattern. This allows for easily swapping in and out different algorithms, which makes testing and optimization of the cache much more practical. In the worst case that there is no unused object available, after traversing the cache you must instantiate a new object and add it to the cache. You probably want to do a bulk allocation of objects when this occurs and add all of them to the cache, preferably with memory mapping. Note that with any cache you want enough, but not too many objects bulk-initialized, because you want to avoid excessively polluting your L1/L2 cache which can lead to thrashing

When retrieving extrinsic objects with immutable state you must simply search the cache for an object with the state you desire, if it exists. This is best done with a hash structure, hashing the state of the object. If you don't find such an object than you will have to initialize a single object with that state. This is another disadvantage to using objects with immutable state - the inability to perform bulk initializations.

When retrieving extrinsic objects with mutable state, you must search the cache for an object with the desired state and search the cache for an unused object to reinitialize if no used object is found. This approach takes more CPU cycles than other approaches, but is more memory efficient as it minimizes the total objects that must be initialized while also sharing them. It is generally preferred. A common mistake in optimizing algorithms is to optimize the CPU instructions at the expense of using more memory. Any retrieval of data from disk takes multiple orders of magnitude longer than a CPU instruction (SSD takes 8 orders of magnitude longer than CPU instruction, for example). Therefor the top priority in optimizing code is generally to minimize the memory footprint. Of course there are exceptions, but they're less common than you'd expect.

You can use separate caches for each unique subclass of extrinsic object. Multiple caches you can optimize separately, associating a unique search algorithm with each cache. Do this based on testing, different caches have different memory usage patterns. Memory usage patterns also change throughout the applications execution. You can change the selection algorithm for a particular cache throughout execution. For example, when the application first runs you should have a zero miss rate, this changes as those initial bulk allocated objects get used. You want to change your selection algorithm at that point. If you encapsulated your selection algorithm in a Strategy pattern this is easy to do.

Restashing objects
The restashing algorithm is quite simple, and easily implemented by overriding the objects deallocation function. If using a maintained cache you merely push the object back onto it within dealloc. If using an unmaintained, unshared cache, assuming you're using a flag to track use, just update the flag in dealloc. If your cache is shared, the objects should have a property that acts like a reference count, and this should be decremented in dealloc. This way you know when the object becomes unused, and is open to being mutated. Otherwise if your language uses automatic reference counting you can just set the pointer to the object to nil and the reference count will automatically be decremented. Your cache search algorithm will then be able to detect the object is unused by checking its reference count.

Encapsulating the caching system
This object caching system - the factory initiating a request, the routing to caches, the traversal / retrieval of a valid object from the cache, the handling of depleted caches, and the objects APIs - can be encapsulated in a Chain of Responsibility pattern. Doing so structures the implementation a way that maintains loose coupling between these various components. Though most request propagation logic should already encapsulated within the cache traversal object, the loose coupling between components can be useful when reconfiguring your implementation (like if you need to modify it to work in a different environment, such as a multithreaded environment; if you want to test different types of caches; if you want to reuse the flyweight for a different set of objects, etc.).

Performing operations on flyweight collections
Common operations on these diverse collections include reinitializing reused extrinsic objects, iterative or single operations with conditional logic, and operations performed on certain subsets of objects. The Visitor design pattern combines very naturally with the flyweight pattern for implementing such behavior. The pattern encapsulates a behavior to be performed by a diverse collection of objects, such as the various extrinsic objects involved in the flyweight pattern. Using visitors in your code will make it better encapsulated, more readable / modifiable, and more memory-efficient than extending classes.